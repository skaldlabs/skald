# Database
DATABASE_URL=postgres://postgres:password@localhost/skald2

# Frontend
FRONTEND_URL=http://localhost:5173
APP_HOST=http://localhost:3000

# Authentication
DISABLE_AUTH=False
EMAIL_VERIFICATION_ENABLED=True
EMAIL_DOMAIN=useskald.com

# Resend (Email Service)
RESEND_API_KEY=your-resend-api-key

# Stripe (Payment Processing)
STRIPE_SECRET_KEY=your-stripe-secret-key
STRIPE_PUBLISHABLE_KEY=your-stripe-publishable-key
STRIPE_WEBHOOK_SECRET=your-stripe-webhook-secret

# Embedding Provider Configuration
# Choose your embedding provider: voyage, openai, or local
EMBEDDING_PROVIDER=voyage

# Voyage AI Embeddings (if using voyage provider)
VOYAGE_API_KEY=your-voyage-api-key
VOYAGE_EMBEDDING_MODEL=voyage-3-large

# OpenAI Embeddings (if using openai provider)
OPENAI_EMBEDDING_MODEL=text-embedding-3-large

# Local Embeddings (if using local provider)
# Requires running the embedding-service (docker-compose --profile local-embedding up)
EMBEDDING_SERVICE_URL=http://localhost:8001

# LLM Configuration
# Choose your LLM provider: openai, anthropic, groq, or local
LLM_PROVIDER=openai

# OpenAI (if using openai provider)
OPENAI_API_KEY=your-openai-api-key
OPENAI_MODEL=gpt-4o-mini

# Anthropic (if using anthropic provider)
ANTHROPIC_API_KEY=your-anthropic-api-key
ANTHROPIC_MODEL=claude-3-7-sonnet-20250219

# Groq (if using groq provider)
GROQ_API_KEY=your-groq-api-key
GROQ_MODEL=llama-3.3-70b-versatile

# Local LLM (if using local provider)
# Works with: Ollama, LM Studio, vLLM, LocalAI, etc.
# Examples:
#   - Ollama: http://localhost:11434/v1
#   - LM Studio: http://localhost:1234/v1
#   - vLLM: http://localhost:8000/v1
LOCAL_LLM_BASE_URL=http://localhost:11434/v1
LOCAL_LLM_MODEL=llama3.1:8b
LOCAL_LLM_API_KEY=not-needed

# PostHog (Analytics)
POSTHOG_PUBLIC_API_KEY=your-posthog-key
POSTHOG_HOST=https://app.posthog.com

# Frontend PostHog (if using Vite)
VITE_PUBLIC_POSTHOG_KEY=your-posthog-key
VITE_PUBLIC_POSTHOG_HOST=https://us.i.posthog.com

# Queue Configuration
# Choose your queue backend: pgmq (recommended), redis, rabbitmq, or sqs
INTER_PROCESS_QUEUE=pgmq

# PGMQ (PostgreSQL Message Queue) - recommended for self-hosted and local development
# Uses the same PostgreSQL database, no additional infrastructure needed
PGMQ_QUEUE_NAME=process_memo
PGMQ_DLQ_NAME=process_memo_dlq
PGMQ_MAX_RETRIES=3

# Redis (legacy - for local development only)
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PUB_SUB_CHANNEL_NAME=process_memo

# RabbitMQ (legacy - for self-hosted deployments)
RABBITMQ_HOST=localhost
RABBITMQ_PORT=5672
RABBITMQ_USER=guest
RABBITMQ_PASSWORD=guest
RABBITMQ_VHOST=/
RABBITMQ_QUEUE_NAME=process_memo

# AWS SQS (for production cloud deployments)
SQS_QUEUE_URL=your-sqs-queue-url
SQS_DLQ_QUEUE_URL=your-sqs-dlq-queue-url
AWS_REGION=us-east-2

# Sentry (Error Tracking) - automatically enabled in production when DEBUG=False
# No configuration needed, uses hardcoded DSN in settings.py

# Document Extraction Provider Configuration
# Choose your document extraction provider: datalab or docling
DOCUMENT_EXTRACTION_PROVIDER=docling

# Datalab API (if using datalab provider)
# Required for cloud deployments that use Datalab for document processing
DATALAB_API_KEY=your-datalab-api-key

# Docling Service (if using docling provider)
# For self-hosted deployments, docling-serve runs locally via docker-compose
# No additional configuration needed when using docker-compose
DOCLING_SERVICE_URL=http://localhost:5001
